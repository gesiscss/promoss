{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMDP Topic Model IPython Wrapper\n",
    "\n",
    "This is a Python class which wraps the Java binaries from the HMDP topic model from the PROMOSS topic modelling toolbox. The *promoss.jar* is expected to be in *../promoss.jar*.\n",
    "\n",
    "## HMDP class\n",
    "\n",
    "The HMDP class contains all the methods required to run the HMDP topic model. \n",
    "\n",
    "### Mandatory parameters\n",
    "It takes two parameters as mandatory parameters:\n",
    "* directory \t\tString. Gives the directory of the texts.txt and groups.txt file.\n",
    "* meta_params\t\tString. Specifies the metadata types and gives the desired clustering. Types of metadata are given separated by semicolons (and correspond to the number of different metadata in the meta.txt file. Possible datatypes are:\n",
    " * G\tGeographical coordinates. The number of desired clusters is specified in brackets, i.e. G(1000) will cluster the documents into 1000 clusters based on the geographical coordinates. (Technical detail: we use EM to fit a mixture of fisher distributions.)\n",
    " * T\tUNIX timestamps (in seconds). The number of clusters (based on binning) is given in brackets, and there can be multiple clusterings based on a binning on the timeline or temporal cycles. This is indicated by a letter followed by the number of desired clusters:\n",
    " * L\tBinning based on the timeline. Example: L1000 gives 1000 bins.\n",
    " * Y\tBinning based on the yearly cycle. Example: L1000 gives 1000 bins.\n",
    " * M\tBinning based on the monthly cycle. Example: L1000 gives 1000 bins.\n",
    " * W\tBinning based on the weekly cycle. Example: L1000 gives 1000 bins.\n",
    " * D\tBinning based on the daily  cycle. Example: L1000 gives 1000 bins.\n",
    " * O\tOrdinal values (numbers)\n",
    " * N\tNominal values (text strings)\n",
    "\n",
    "### Optional parameters\n",
    "Additionally, optional parameters can be given. The most commonly used ones are *T, RUNS, processed, stemming, stopwords* and *language*.\n",
    "* T\t\t\tInteger. Number of truncated topics. Default: 100\n",
    "* RUNS\t\t\tInteger. Number of iterations the sampler will run. Default: 200\n",
    "* SAVE_STEP\t\tInteger. Number of iterations after which the learned paramters are saved. Default: 10\n",
    "* TRAINING_SHARE\t\tDouble. Gives the share of documents which are used for training (0 to 1). Default: 1\n",
    "* BATCHSIZE\t\tInteger. Batch size for topic estimation. Default: 128\n",
    "* BATCHSIZE_GROUPS\tInteger. Batch size for group-specific parameter estimation. Default: BATCHSIZE\n",
    "* BURNIN\t\t\tInteger. Number of iterations till the topics are updated. Default: 0\n",
    "* BURNIN_DOCUMENTS\tInteger. Gives the number of sampling iterations where the group-specific parameters are not updated yet. Default: 0\n",
    "* INIT_RAND\t\tDouble. Topic-word counts are initiatlised as INIT_RAND * RANDOM(). Default: 0\n",
    "* SAMPLE_ALPHA\t\tInteger. Every SAMPLE_ALPHAth document is used to estimate alpha_1. Default: 1\n",
    "* BATCHSIZE_ALPHA\tInteger. How many observations do we take before updating alpha_1. Default: 1000\n",
    "* MIN_DICT_WORDS\t\tInteger. If the words.txt file is missing, words.txt is created by using words which occur at least MIN_DICT_WORDS times in the corpus. Default: 100\n",
    "* save_prefix\t\tString. If given, this String is appended to all output files.\n",
    "* alpha_0\t\tDouble. Initial value of alpha_0. Default: 1\n",
    "* alpha_1\t\tDouble. Initial value of alpha_1. Default: 1\n",
    "* epsilon\t\tComma-separated double. Dirichlet prior over the weights of contexts. Comma-separated double values, with dimensionality equal to the number of contexts.\n",
    "* delta_fix \t\tIf set, delta is fixed and set to this value. Otherwise delta is learned during inference.\n",
    "* rhokappa\t\tDouble. Initial value of kappa, a parameter for the learning rate of topics. Default: 0.5\n",
    "* rhotau\t\t\tInteger. Initial value of tau, a parameter for the learning rate of topics. Default: 64\n",
    "* rhos\t\t\tInteger. Initial value of s, a parameter for the learning rate of topics. Default: 1\n",
    "* rhokappa_document\tDouble. Initial value of kappa, a parameter for the learning rate of the document-topic distribution. Default: kappa\n",
    "* rhotau_document\tInteger. Initial value of tau, a parameter for the learning rate of the document-topic distribution. Default: tau\n",
    "* rhos_document\t\tInteger. Initial value of tau, a parameter for the learning rate of the document-topic distribution. Default: rhos\n",
    "* rhokappa_group\t\tDouble. Initial value of kappa, a parameter for the learning rate of the group-topic distribution. Default: kappa\n",
    "* rhotau_group\t\tInteger. Initial value of tau, a parameter for the learning rate of the group-topic distribution. Default: tau\n",
    "* rhos_group\t\tInteger. Initial value of tau, a parameter for the learning rate of the group-topic distribution. Default: rhos\n",
    "* processed\t\tBoolean. Tells if the text is already processed, or if words should be split with complex regular expressions. Otherwise split by spaces. Default: true.\n",
    "* stemming\t\tBoolean. Activates word stemming in case no words.txt/wordsets file is given. Default: false\n",
    "* stopwords\t\tBoolean. Activates stopword removal in case no words.txt/wordsets file is given. Default: false\n",
    "* language\t\tString. Currently \"en\" and \"de\" are available languages for stemming. Default: \"en\"\n",
    "* store_empty\t\tBoolean. Determines if empty documents should be omitted in the final document-topic matrix or if the topic distribution should be predicted using the context. Default: True\n",
    "* topk\t\t\tInteger. Set the number of top words returned in the topktopics file of the output. Default: 100\n",
    "* gamma\t\t\tDouble. Initial scaling parameter of the top-level Dirichlet process. Default: 1\n",
    "* learn_gamma\t\tBoolean. Should gamma be learned during inference? Default: True\n",
    "\n",
    "### Provided methods\n",
    "\n",
    "#### run()\n",
    "This method executes the java binaries with the parameters specified in the initialisation step.\n",
    "\n",
    "#### check_run()\n",
    "Checks if the HMDP model was already trained.\n",
    "\n",
    "*Output: Boolean\n",
    "\n",
    "#### map_from_JSON()\n",
    "Creates HTML files with interactive maps showing the topic probabilities per cluster for all geographical metadata.\n",
    "<img src=\"img/screenshot_map.png\" style=\"height: 300px\" />\n",
    "*Input: \n",
    " * color: Gives the color of the markers (hexadecimal, e.g. #aa23cc). Default: auto (changing colours)\n",
    " * marker_size: Integer, size of markers. Default: 10\n",
    " * show_map: Show map in the IPython notebook. Warning, this can crash your browser. Default: false\n",
    "\n",
    "\n",
    "#### plot_zeta()\n",
    "Show metadata (feature) weights.\n",
    "\n",
    "#### plot_time()\n",
    "Plot temporal distribution(s) of topic probabilities for a given topic.\n",
    "\n",
    "* Input: ID of a topic\n",
    "\n",
    "#### plot_ordinal()\n",
    "Plot distribution of topic probabilities over ordinal metadata variables for a given topic.\n",
    "* Input: ID of a topic\n",
    "\n",
    "\n",
    "#### get_topics()\n",
    "* Output: Returns the top-k words (k given by parameter -topk of the HMDP class) in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import io, os, shutil, time, datetime\n",
    "import subprocess\n",
    "import folium\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import IFrame, display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class HMDP(object):\n",
    "    directory = \"\";\n",
    "    meta_params = \"\";\n",
    "    T=100\n",
    "    RUNS=200\n",
    "    SAVE_STEP=10\n",
    "    TRAINING_SHARE=1.0\n",
    "    BATCHSIZE=128\n",
    "    BATCHSIZE_GROUPS=128\n",
    "    BURNIN=0\n",
    "    BURNIN_DOCUMENTS=0\n",
    "    INIT_RAND=0\n",
    "    SAMPLE_ALPHA=1\n",
    "    BATCHSIZE_ALPHA=1000\n",
    "    MIN_DICT_WORDS=100\n",
    "    alpha_0=1\n",
    "    alpha_1=1\n",
    "    epsilon=\"none\"\n",
    "    delta_fix=\"none\"\n",
    "    rhokappa=0.5\n",
    "    rhotau=64\n",
    "    rhos=1\n",
    "    rhokappa_document=0.5\n",
    "    rhotau_document=64\n",
    "    rhos_document=1\n",
    "    rhokappa_group=0.5\n",
    "    rhotau_group=64\n",
    "    rhos_group=1\n",
    "    processed=True\n",
    "    stemming=False\n",
    "    stopwords=False\n",
    "    language=\"en\"\n",
    "    store_empty=True\n",
    "    topk=100\n",
    "    gamma = 1\n",
    "    learn_gamma = True;\n",
    "    \n",
    "    def __init__(self,\n",
    "    directory,\n",
    "    meta_params,\n",
    "    T=100,\n",
    "    RUNS=200,\n",
    "    SAVE_STEP=10,\n",
    "    TRAINING_SHARE=1.0,\n",
    "    BATCHSIZE=128,\n",
    "    BATCHSIZE_GROUPS=128,\n",
    "    BURNIN=0,\n",
    "    BURNIN_DOCUMENTS=0,\n",
    "    INIT_RAND=0,\n",
    "    SAMPLE_ALPHA=1,\n",
    "    BATCHSIZE_ALPHA=1000,\n",
    "    MIN_DICT_WORDS=100,\n",
    "    alpha_0=1,\n",
    "    alpha_1=1,\n",
    "    epsilon=\"none\",\n",
    "    delta_fix=\"none\",\n",
    "    rhokappa=0.5,\n",
    "    rhotau=64,\n",
    "    rhos=1,\n",
    "    rhokappa_document=0.5,\n",
    "    rhotau_document=64,\n",
    "    rhos_document=1,\n",
    "    rhokappa_group=0.5,\n",
    "    rhotau_group=64,\n",
    "    rhos_group=1,\n",
    "    processed=True,\n",
    "    stemming=False,\n",
    "    stopwords=False,\n",
    "    language=\"en\",\n",
    "    store_empty=True,\n",
    "    topk=100,\n",
    "    gamma = 1,\n",
    "    learn_gamma = True\n",
    "                ):\n",
    "        self.directory = directory\n",
    "        self.meta_params = meta_params\n",
    "        self.T = T\n",
    "        self.RUNS = RUNS\n",
    "        self.SAVE_STEP = SAVE_STEP\n",
    "        self.TRAINING_SHARE = TRAINING_SHARE\n",
    "        self.BATCHSIZE = BATCHSIZE\n",
    "        self.BATCHSIZE_GROUPS = BATCHSIZE_GROUPS\n",
    "        self.BURNIN = BURNIN\n",
    "        self.BURNIN_DOCUMENTS = BURNIN_DOCUMENTS\n",
    "        self.INIT_RAND = INIT_RAND\n",
    "        self.SAMPLE_ALPHA = SAMPLE_ALPHA\n",
    "        self.BATCHSIZE_ALPHA = BATCHSIZE_ALPHA\n",
    "        self.MIN_DICT_WORDS = MIN_DICT_WORDS\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.alpha_1 = alpha_1\n",
    "        self.epsilon = epsilon\n",
    "        self.delta_fix = delta_fix\n",
    "        self.rhokappa = rhokappa\n",
    "        self.rhotau = rhotau\n",
    "        self.rhos = rhos\n",
    "        self.rhokappa_document = rhokappa_document\n",
    "        self.rhotau_document = rhotau_document\n",
    "        self.rhos_document = rhos_document\n",
    "        self.rhokappa_group = rhokappa_group\n",
    "        self.rhotau_group = rhotau_group\n",
    "        self.rhos_group = rhos_group\n",
    "        self.processed = processed\n",
    "        self.stemming = stemming\n",
    "        self.stopwords = stopwords\n",
    "        self.language = language\n",
    "        self.store_empty = store_empty\n",
    "        self.topk = topk\n",
    "        self.gamma = gamma\n",
    "        self.learn_gamma = learn_gamma\n",
    "\n",
    "    def run(self, RUNS = None):\n",
    "        if RUNS == None:\n",
    "            RUNS = self.RUNS;\n",
    "            \n",
    "        print(\"Running HMDP topic model... (please wait)\");\n",
    "\n",
    "        if os.path.isdir(directory+\"/output_HMDP\"):\n",
    "            shutil.rmtree(directory+\"/output_HMDP\") \n",
    "        if os.path.isdir(self.directory+\"/cluster_desc\"):\n",
    "            shutil.rmtree(self.directory+\"/cluster_desc\") \n",
    "\n",
    "        if os.path.isfile(self.directory+\"/groups\"):\n",
    "            os.remove(self.directory+\"/groups\")\n",
    "        if os.path.isfile(self.directory+\"/groups.txt\"):\n",
    "            os.remove(self.directory+\"/groups.txt\")\n",
    "        if os.path.isfile(self.directory+\"/text.txt\"):\n",
    "            os.remove(self.directory+\"/text.txt\")\n",
    "        if os.path.isfile(self.directory+\"/words.txt\"):\n",
    "            os.remove(self.directory+\"/words.txt\")\n",
    "        if os.path.isfile(self.directory+\"/wordsets\"):\n",
    "            os.remove(self.directory+\"/wordsets\")\n",
    "\n",
    "        if not os.path.isfile(\"../promoss.jar\"):\n",
    "            print(\"Could not find ../promoss.jar. Exit\")\n",
    "            return;\n",
    "        try:\n",
    "            with subprocess.Popen(['java', '-jar', '../promoss.jar', \n",
    "                                '-directory', self.directory, \n",
    "                                '-meta_params', self.meta_params, \n",
    "                                '-T',str(self.T),\n",
    "                                '-RUNS',str(self.RUNS),\n",
    "                                '-SAVE_STEP',str(self.SAVE_STEP),\n",
    "                                '-TRAINING_SHARE',str(self.TRAINING_SHARE),\n",
    "                                '-BATCHSIZE',str(self.BATCHSIZE),\n",
    "                                '-BATCHSIZE_GROUPS',str(self.BATCHSIZE_GROUPS),\n",
    "                                '-BURNIN',str(self.BURNIN),\n",
    "                                '-BURNIN_DOCUMENTS',str(self.BURNIN_DOCUMENTS),\n",
    "                                '-INIT_RAND',str(self.INIT_RAND),\n",
    "                                '-SAMPLE_ALPHA',str(self.SAMPLE_ALPHA),\n",
    "                                '-BATCHSIZE_ALPHA',str(self.BATCHSIZE_ALPHA),\n",
    "                                '-MIN_DICT_WORDS',str(self.MIN_DICT_WORDS),\n",
    "                                '-alpha_0',str(self.alpha_0),\n",
    "                                '-alpha_1',str(self.alpha_1),\n",
    "                                '-epsilon',str(self.epsilon),\n",
    "                                '-delta_fix',str(self.delta_fix),\n",
    "                                '-rhokappa',str(self.rhokappa),\n",
    "                                '-rhotau',str(self.rhotau),\n",
    "                                '-rhos',str(self.rhos),\n",
    "                                '-rhokappa_document',str(self.rhokappa_document),\n",
    "                                '-rhotau_document',str(self.rhotau_document),\n",
    "                                '-rhos_document',str(self.rhos_document),\n",
    "                                '-rhokappa_group',str(self.rhokappa_group),\n",
    "                                '-rhotau_group',str(self.rhotau_group),\n",
    "                                '-rhos_group',str(self.rhos_group),\n",
    "                                '-processed',str(self.processed),\n",
    "                                '-stemming',str(self.stemming),\n",
    "                                '-stopwords',str(self.stopwords),\n",
    "                                '-language',str(self.language),\n",
    "                                '-store_empty',str(self.store_empty),\n",
    "                                '-topk',str(self.topk),\n",
    "                                '-gamma',str(self.gamma),\n",
    "                                '-learn_gamma',str(self.learn_gamma)\n",
    "                                ], stdout=subprocess.PIPE, stderr=subprocess.PIPE) as p:   \n",
    "\n",
    "                for line in p.stdout:\n",
    "                    line = str(line)[2:-1].replace(\"\\\\n\",\"\").replace(\"\\\\t\",\"   \")\n",
    "                    print(line, end='\\n');\n",
    "                for line in p.stderr:\n",
    "                    line = str(line)[2:-1].replace(\"\\\\n\",\"\").replace(\"\\\\t\",\"   \")\n",
    "                    print(line, end='\\n');\n",
    "\n",
    "                \n",
    "            #rc = process.poll();\n",
    "            #print(\"Finished with return code \" + str(rc));\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(e.returncode)\n",
    "            print(e.output)\n",
    "\n",
    "    def check_run(self):\n",
    "        if os.path.isdir(self.directory + \"/output_HMDP/\" + str(self.RUNS)):\n",
    "            return True;\n",
    "        else:\n",
    "            print(\"Please call run() first\");\n",
    "            return False;\n",
    "            \n",
    "            \n",
    "    #returns the command which we used to call the java file\n",
    "    def get_command(self):\n",
    "        args = ['java', '-jar', '../promoss.jar', \n",
    "                                '-directory', self.directory, \n",
    "                                '-meta_params', self.meta_params, \n",
    "                                '-T',str(self.T),\n",
    "                                '-RUNS',str(self.RUNS),\n",
    "                                '-SAVE_STEP',str(self.SAVE_STEP),\n",
    "                                '-TRAINING_SHARE',str(self.TRAINING_SHARE),\n",
    "                                '-BATCHSIZE',str(self.BATCHSIZE),\n",
    "                                '-BATCHSIZE_GROUPS',str(self.BATCHSIZE_GROUPS),\n",
    "                                '-BURNIN',str(self.BURNIN),\n",
    "                                '-BURNIN_DOCUMENTS',str(self.BURNIN_DOCUMENTS),\n",
    "                                '-INIT_RAND',str(self.INIT_RAND),\n",
    "                                '-SAMPLE_ALPHA',str(self.SAMPLE_ALPHA),\n",
    "                                '-BATCHSIZE_ALPHA',str(self.BATCHSIZE_ALPHA),\n",
    "                                '-MIN_DICT_WORDS',str(self.MIN_DICT_WORDS),\n",
    "                                '-alpha_0',str(self.alpha_0),\n",
    "                                '-alpha_1',str(self.alpha_1),\n",
    "                                '-epsilon',str(self.epsilon),\n",
    "                                '-delta_fix',str(self.delta_fix),\n",
    "                                '-rhokappa',str(self.rhokappa),\n",
    "                                '-rhotau',str(self.rhotau),\n",
    "                                '-rhos',str(self.rhos),\n",
    "                                '-rhokappa_document',str(self.rhokappa_document),\n",
    "                                '-rhotau_document',str(self.rhotau_document),\n",
    "                                '-rhos_document',str(self.rhos_document),\n",
    "                                '-rhokappa_group',str(self.rhokappa_group),\n",
    "                                '-rhotau_group',str(self.rhotau_group),\n",
    "                                '-rhos_group',str(self.rhos_group),\n",
    "                                '-processed',str(self.processed),\n",
    "                                '-stemming',str(self.stemming),\n",
    "                                '-stopwords',str(self.stopwords),\n",
    "                                '-language',str(self.language),\n",
    "                                '-store_empty',str(self.store_empty),\n",
    "                                '-topk',str(self.topk)\n",
    "                                ];\n",
    "        return (\" \".join(args));\n",
    "        \n",
    "    #function to create topic maps based on JSON files by the HMDP topic model\n",
    "    def map_from_JSON(self, base_folder = None, runs = None, color='auto', marker_size=10, show_map=False):\n",
    "\n",
    "        if not self.check_run():\n",
    "            return;\n",
    "        \n",
    "        if base_folder == None:\n",
    "            base_folder = self.directory;\n",
    "        if runs == None:\n",
    "            runs = self.RUNS;\n",
    "            \n",
    "        topics = self.get_topics();\n",
    "        k = 3;\n",
    "        \n",
    "        #we only create a map for the final run folder.\n",
    "        #comment the next line to create maps for all folders\n",
    "        final_run_folder = base_folder + \"/output_HMDP/\" + str(runs) +\"/\";\n",
    "\n",
    "        #traverse folders containing geojson files\n",
    "        folders = [x[0] for x in os.walk(final_run_folder) if x[0].endswith(\"_geojson\")];\n",
    "        \n",
    "        if (len(folders)==0):\n",
    "            print(\"No geoJSON data found. Does your model contain geographical metadata?\");\n",
    "            return;\n",
    "        \n",
    "        for folder in folders:\n",
    "            print(\"opening folder \"+folder+\":\");\n",
    "\n",
    "            #Create new folium map class\n",
    "            f_map = folium.Map(location=[50, 6], tiles='Stamen Toner', zoom_start=1);\n",
    "\n",
    "            #traverse geoJSON files\n",
    "            files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) & f.endswith(\".geojson\")];\n",
    "            \n",
    "            topic_numbers = [\"\"]*len(files);\n",
    "            \n",
    "            for i in range(0,len(files)):\n",
    "                topic_numbers[i] = int(files[i].split(\"_\")[1].split(\".\")[0]);\n",
    "           \n",
    "            files = [x for (y,x) in sorted(zip(topic_numbers,files))]\n",
    "            \n",
    "            i = 0;\n",
    "            for file in files:\n",
    "                print(\"processing \"+file+\" ...\");\n",
    "\n",
    "                with open(folder+'/'+file) as f:\n",
    "                    geojson = json.load(f)\n",
    "\n",
    "                icon_size = (14, 14)\n",
    "\n",
    "                #name of the topic are the first three topic words\n",
    "                name = \"Topic \"+str(i)+\": \"+\" \".join(topics.iloc[i][0:k]);        \n",
    "                #traverse geoJSON features\n",
    "                feature_group = folium.FeatureGroup(name);\n",
    "                for feature in geojson['features']:\n",
    "                    #we get position, colour, transparency from JSON\n",
    "                    lat, lon = feature['geometry']['coordinates'];\n",
    "                    if color == 'auto':\n",
    "                        fillColor = \"#\"+feature['properties']['fillColor'];\n",
    "                    else:\n",
    "                        fillColor = color;\n",
    "                    fillOpacity = feature['properties']['fillOpacity'];\n",
    "                    marker = folium.CircleMarker([lat, lon], \n",
    "                                                 fill_color=fillColor, \n",
    "                                                 fill_opacity=fillOpacity,\n",
    "                                                 color = \"none\",\n",
    "                                                 radius = marker_size)\n",
    "                    feature_group.add_child(marker);\n",
    "\n",
    "                f_map.add_child(feature_group);\n",
    "                f.close();\n",
    "                i=i+1;\n",
    "\n",
    "            #add layer control to activate/deactivate topics\n",
    "            folium.LayerControl().add_to(f_map);    \n",
    "            #save map\n",
    "            f_map.save(folder+'/topic_map.htm')\n",
    "            print('created map in: '+folder+'/topic_map.htm');\n",
    "            f_map._repr_html_();\n",
    "            #show map only if wanted, can consume quite some memory\n",
    "            if show_map:\n",
    "                if not os.path.exists(\"tmp\"):\n",
    "                    os.makedirs(\"tmp\");\n",
    "\n",
    "                f_map.save(\"tmp/\"+folder.split(\"/\")[-1]+\"_map.html\");\n",
    "                display(IFrame(\"tmp/\"+folder.split(\"/\")[-1]+\"_map.html\",width=400, height=400));\n",
    "                display(f_map._repr_png());\n",
    "            display(HTML('<a href=\"file://'+folder+'/topic_map.htm'+'\" target=\"_blank\">Link to map of '+folder.split(\"/\")[-1].replace(\"_geojson\",\"\")+'</a>'));\n",
    "\n",
    "    #plot topic proportions\n",
    "    def plot_zeta(self, directory=None, RUNS=None):\n",
    "        \n",
    "        if not self.check_run():\n",
    "            return;\n",
    "        \n",
    "        if directory == None:\n",
    "            directory = self.directory;\n",
    "        if RUNS == None:\n",
    "            RUNS = self.RUNS;\n",
    "            \n",
    "        fig = plt.figure();\n",
    "        \n",
    "        zeta_file = self.directory + \"/output_HMDP/\" + str(RUNS) +\"/zeta\";\n",
    "        \n",
    "        df = pd.read_csv(zeta_file, header=None);\n",
    "        zeta = df.iloc[[0]].values[0];\n",
    "        print(zeta);\n",
    "               \n",
    "        plt.bar(range(0,len(zeta)),zeta);\n",
    "        plt.xticks(range(0,len(zeta)));\n",
    "        plt.xlabel(\"Features\");\n",
    "        plt.ylabel(\"Feature weight\");\n",
    "        plt.show();\n",
    "        \n",
    "        return(fig);\n",
    "    \n",
    "    #read topics ad DataFrame\n",
    "    def get_topics(self, directory=None, RUNS=None):\n",
    "        \n",
    "        if not self.check_run():\n",
    "            return;\n",
    "        \n",
    "        if directory == None:\n",
    "            directory = self.directory;\n",
    "        if RUNS == None:\n",
    "            RUNS = self.RUNS;\n",
    "            \n",
    "        \n",
    "        topic_file = self.directory + \"/output_HMDP/\" + str(RUNS) +\"/topktopic_words\";\n",
    "        \n",
    "        df = pd.read_csv(topic_file, header=None, sep=\" \");\n",
    "                \n",
    "        return(df);\n",
    "    \n",
    "    #plot topic probabilities over time\n",
    "    def plot_time(self, topic_ID, directory=None, RUNS=None):\n",
    "        \n",
    "        if not self.check_run():\n",
    "            return;\n",
    "        \n",
    "        if directory == None:\n",
    "            directory = self.directory;\n",
    "        if RUNS == None:\n",
    "            RUNS = self.RUNS;           \n",
    "            \n",
    "        topics = self.get_topics();\n",
    "        k = min(3,len(topics.iloc[0]));\n",
    "        \n",
    "        #traverse folders containing time files\n",
    "        time_files = [x for x in os.listdir(directory+\"/cluster_desc/\") if x.endswith(\"_L\")];\n",
    "\n",
    "       \n",
    "        figs = [];\n",
    "        \n",
    "        for time_file in time_files:\n",
    "            \n",
    "            time_file = directory+\"/cluster_desc/\"+time_file;\n",
    "            \n",
    "            cluster_number = int(time_file.split(\"/\")[-1][7:-2]);\n",
    "        \n",
    "            times = pd.read_csv(time_file, header=None, sep=\" \");\n",
    "            times = times[1];\n",
    "            \n",
    "            #print(times);\n",
    "            \n",
    "            first_time = min(times);\n",
    "            last_time = max(times);            \n",
    "            \n",
    "            first_date = datetime.datetime.fromtimestamp(\n",
    "                    int(first_time)\n",
    "                    ).strftime('%d.%m.%Y');            \n",
    "            last_date = datetime.datetime.fromtimestamp(\n",
    "                    int(last_time)\n",
    "                    ).strftime('%d.%m.%Y');\n",
    "\n",
    "            fig = plt.figure();\n",
    "\n",
    "            cluster_file = self.directory + \"/output_HMDP/\" + str(RUNS) +\"/clusters_\"+str(cluster_number);\n",
    "           \n",
    "            probabilities = pd.read_csv(cluster_file, header=None);\n",
    "            topic_probabilities = probabilities[topic_ID];\n",
    "\n",
    "            topic_probabilities = [x for (y,x) in sorted(zip(times,topic_probabilities))]\n",
    "            times = sorted(times);\n",
    "\n",
    "            \n",
    "            #name of the topic are the first three topic words\n",
    "            name = \"Topic \"+str(topic_ID)+\": \"+\" \".join(topics.iloc[topic_ID][0:k]);\n",
    "            \n",
    "            fig = plt.figure();\n",
    "            \n",
    "            #print(times)\n",
    "            #print(topic_probabilities)\n",
    "            \n",
    "            plt.scatter(times, topic_probabilities);\n",
    "            plt.xticks([first_time,last_time],[first_date,last_date]);\n",
    "            plt.xlabel(\"Time\");\n",
    "            plt.ylabel(\"Topic probability\");\n",
    "            plt.legend([name]);\n",
    "            plt.show();\n",
    "            figs.append(fig);\n",
    "        \n",
    "        return(figs);\n",
    "    \n",
    "    #plot topic probabilities for ordinal data\n",
    "    def plot_ordinal(self, topic_ID, directory=None, RUNS=None):\n",
    "        \n",
    "        if not self.check_run():\n",
    "            return;\n",
    "        \n",
    "        if directory == None:\n",
    "            directory = self.directory;\n",
    "        if RUNS == None:\n",
    "            RUNS = self.RUNS;           \n",
    "            \n",
    "        topics = self.get_topics();\n",
    "        k = min(3,len(topics.iloc[0]));\n",
    "        \n",
    "        #traverse folders containing time files\n",
    "        cluster_files = [x for x in os.listdir(directory+\"/cluster_desc/\") if x.endswith(\"_O\")];\n",
    "       \n",
    "        figs = [];\n",
    "        \n",
    "        for cluster_file in cluster_files:\n",
    "            \n",
    "            cluster_file = directory+\"/cluster_desc/\"+cluster_file;\n",
    "            \n",
    "            cluster_number = int(cluster_file.split(\"/\")[-1][7:-2]);\n",
    "        \n",
    "            lines = pd.read_csv(cluster_file, header=None,names=[\"keys\",\"values\"], skiprows=1, sep=\" \");\n",
    "            keys = lines[\"keys\"].values;\n",
    "            values = lines[\"values\"].values;\n",
    "            \n",
    "            #sort by keys\n",
    "            [keys,values] = list(zip(*sorted(zip(keys,values))));\n",
    "            \n",
    "            #print(times);\n",
    "            \n",
    "            fig = plt.figure();\n",
    "\n",
    "            cluster_file = self.directory + \"/output_HMDP/\" + str(RUNS) +\"/clusters_\"+str(cluster_number);\n",
    "           \n",
    "            probabilities = pd.read_csv(cluster_file, header=None);\n",
    "            topic_probabilities = probabilities[topic_ID];\n",
    "            topic_probabilities = [x for (y,x) in sorted(zip(values,topic_probabilities))]\n",
    "            \n",
    "            #name of the topic are the first three topic words\n",
    "            name = \"Topic \"+str(topic_ID)+\": \"+\" \".join(topics.iloc[topic_ID][0:k]);\n",
    "            \n",
    "            fig = plt.figure();\n",
    "            \n",
    "            #print(times)\n",
    "            #print(topic_probabilities)\n",
    "            \n",
    "            value_array = [];\n",
    "            value_array.append(values);\n",
    "            value_array = [x for xs in value_array for x in xs];\n",
    "            #print(value_array);\n",
    "            #print(topic_probabilities);\n",
    "            \n",
    "            plt.scatter(value_array, topic_probabilities);\n",
    "            plt.xticks(values,keys);\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.xlabel(\"Category\");\n",
    "            plt.ylabel(\"Topic probability\");\n",
    "            plt.legend([name]);\n",
    "            plt.show();\n",
    "            figs.append(fig);\n",
    "        \n",
    "        return(figs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
